{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f9145b",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from numpy import zeros, newaxis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from common.data_loader import *\n",
    "from common.preprocessing import*\n",
    "\n",
    "from os import path, environ\n",
    "import matplotlib.pyplot as plt\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34f1d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eea9c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010f2970",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4228623 entries, 0 to 4228622\n",
      "Data columns (total 15 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   TurbID   int64  \n",
      " 1   Day      int64  \n",
      " 2   Tmstamp  object \n",
      " 3   Wspd     float64\n",
      " 4   Wdir     float64\n",
      " 5   Etmp     float64\n",
      " 6   Itmp     float64\n",
      " 7   Ndir     float64\n",
      " 8   Pab1     float64\n",
      " 9   Pab2     float64\n",
      " 10  Pab3     float64\n",
      " 11  Prtv     float64\n",
      " 12  Patv     float64\n",
      " 13  Date     object \n",
      " 14  Time     int64  \n",
      "dtypes: float64(10), int64(3), object(2)\n",
      "memory usage: 483.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "full_data = pd.read_csv(\"data/data5_gi.csv\")\n",
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bab9143",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TurbID</th>\n",
       "      <th>Day</th>\n",
       "      <th>Tmstamp</th>\n",
       "      <th>Wspd</th>\n",
       "      <th>Wdir</th>\n",
       "      <th>Etmp</th>\n",
       "      <th>Itmp</th>\n",
       "      <th>Ndir</th>\n",
       "      <th>Pab1</th>\n",
       "      <th>Pab2</th>\n",
       "      <th>Pab3</th>\n",
       "      <th>Prtv</th>\n",
       "      <th>Patv</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00</td>\n",
       "      <td>6.17</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>30.73</td>\n",
       "      <td>41.80</td>\n",
       "      <td>25.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>494.66</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00:10</td>\n",
       "      <td>6.17</td>\n",
       "      <td>-3.99</td>\n",
       "      <td>30.73</td>\n",
       "      <td>41.80</td>\n",
       "      <td>25.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>494.66</td>\n",
       "      <td>2021-01-01 00:10:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00:20</td>\n",
       "      <td>6.27</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>30.60</td>\n",
       "      <td>41.63</td>\n",
       "      <td>20.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>509.76</td>\n",
       "      <td>2021-01-01 00:20:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00:30</td>\n",
       "      <td>6.42</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>30.52</td>\n",
       "      <td>41.52</td>\n",
       "      <td>20.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>542.53</td>\n",
       "      <td>2021-01-01 00:30:00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00:40</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.89</td>\n",
       "      <td>30.49</td>\n",
       "      <td>41.38</td>\n",
       "      <td>20.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>509.36</td>\n",
       "      <td>2021-01-01 00:40:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228618</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:10</td>\n",
       "      <td>4.58</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>3.37</td>\n",
       "      <td>194.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-51.69</td>\n",
       "      <td>238.01</td>\n",
       "      <td>2021-08-10 23:10:00</td>\n",
       "      <td>31964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228619</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:20</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>3.16</td>\n",
       "      <td>187.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-31.13</td>\n",
       "      <td>172.71</td>\n",
       "      <td>2021-08-10 23:20:00</td>\n",
       "      <td>31965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228620</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:30</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>2.92</td>\n",
       "      <td>187.96</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-58.67</td>\n",
       "      <td>157.63</td>\n",
       "      <td>2021-08-10 23:30:00</td>\n",
       "      <td>31966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228621</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:40</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.13</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>2.81</td>\n",
       "      <td>191.76</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-56.66</td>\n",
       "      <td>244.92</td>\n",
       "      <td>2021-08-10 23:40:00</td>\n",
       "      <td>31967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228622</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:50</td>\n",
       "      <td>5.36</td>\n",
       "      <td>4.86</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>2.80</td>\n",
       "      <td>198.52</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-50.31</td>\n",
       "      <td>337.33</td>\n",
       "      <td>2021-08-10 23:50:00</td>\n",
       "      <td>31968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4228623 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TurbID  Day Tmstamp  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  Pab3   Prtv    Patv  \\\n",
       "0             1    1   00:00  6.17 -3.99  30.73  41.80   25.92  1.00  1.00  1.00  -0.25  494.66   \n",
       "1             1    1   00:10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00  1.00  -0.25  494.66   \n",
       "2             1    1   00:20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00  1.00  -0.24  509.76   \n",
       "3             1    1   00:30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00  1.00  -0.26  542.53   \n",
       "4             1    1   00:40  6.25  0.89  30.49  41.38   20.91  1.00  1.00  1.00  -0.23  509.36   \n",
       "...         ...  ...     ...   ...   ...    ...    ...     ...   ...   ...   ...    ...     ...   \n",
       "4228618     134  222   23:10  4.58 -4.41  -0.99   3.37  194.37  0.01  0.01  0.01 -51.69  238.01   \n",
       "4228619     134  222   23:20  3.74  0.88  -1.05   3.16  187.96  0.01  0.01  0.01 -31.13  172.71   \n",
       "4228620     134  222   23:30  3.54  0.34  -1.21   2.92  187.96  0.03  0.03  0.03 -58.67  157.63   \n",
       "4228621     134  222   23:40  4.46  4.13  -1.24   2.81  191.76  0.03  0.03  0.03 -56.66  244.92   \n",
       "4228622     134  222   23:50  5.36  4.86  -1.06   2.80  198.52  0.06  0.06  0.06 -50.31  337.33   \n",
       "\n",
       "                        Date   Time  \n",
       "0        2021-01-01 00:00:00      1  \n",
       "1        2021-01-01 00:10:00      2  \n",
       "2        2021-01-01 00:20:00      3  \n",
       "3        2021-01-01 00:30:00      4  \n",
       "4        2021-01-01 00:40:00      5  \n",
       "...                      ...    ...  \n",
       "4228618  2021-08-10 23:10:00  31964  \n",
       "4228619  2021-08-10 23:20:00  31965  \n",
       "4228620  2021-08-10 23:30:00  31966  \n",
       "4228621  2021-08-10 23:40:00  31967  \n",
       "4228622  2021-08-10 23:50:00  31968  \n",
       "\n",
       "[4228623 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83245780",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f13a90c",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Data name: Feature engineering\n",
      "  - Number of data: 4228623\n",
      "  - Number of nan rows: 864\n"
     ]
    }
   ],
   "source": [
    "full_data = feature_engineering(full_data, compute_Pmax_method ='clipping', compute_Pmax_clipping=False)\n",
    "full_data = marking_data(full_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5369d3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## feature selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc464efb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## n to patv prediction\n",
    "features = ['TurbID', 'Day', 'Tmstamp','PatvMA3','WspdMA3', 'RPMMA3','Patv5','Wspd5', 'RPM5','locX', 'locY','DayX', 'DayY', 'WdirX', 'WdirY','Wspd', 'Wspd_cube','Etmp_abs','Pab1', 'Pab2', 'Pab3', 'Bspd1', 'Bspd3', 'Bspd2','Prtv','Patv']\n",
    "full_data_selected = full_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86cab9b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## n to wspd predicion and calculate patv\n",
    "features = ['TurbID', 'Day', 'Tmstamp','WspdMA3','Wspd5', 'locX', 'locY','DayX', 'DayY','Weekday', 'WdirX', 'WdirY', 'Wspd_cube','Etmp_abs','Wspd']\n",
    "full_data_selected = full_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc24fbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TurbID</th>\n",
       "      <th>Day</th>\n",
       "      <th>Tmstamp</th>\n",
       "      <th>WspdMA3</th>\n",
       "      <th>Wspd5</th>\n",
       "      <th>locX</th>\n",
       "      <th>locY</th>\n",
       "      <th>DayX</th>\n",
       "      <th>DayY</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>WdirX</th>\n",
       "      <th>WdirY</th>\n",
       "      <th>Wspd_cube</th>\n",
       "      <th>Etmp_abs</th>\n",
       "      <th>Wspd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>00:00</td>\n",
       "      <td>10.235417</td>\n",
       "      <td>6.17</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.994678</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.958770</td>\n",
       "      <td>-0.284183</td>\n",
       "      <td>229.130355</td>\n",
       "      <td>264.20</td>\n",
       "      <td>6.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>00:10</td>\n",
       "      <td>10.226273</td>\n",
       "      <td>6.17</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.994678</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.960050</td>\n",
       "      <td>-0.279829</td>\n",
       "      <td>339.998645</td>\n",
       "      <td>264.05</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>00:20</td>\n",
       "      <td>10.222338</td>\n",
       "      <td>6.27</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.994678</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.945916</td>\n",
       "      <td>-0.324413</td>\n",
       "      <td>722.854363</td>\n",
       "      <td>263.95</td>\n",
       "      <td>8.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>00:30</td>\n",
       "      <td>10.215301</td>\n",
       "      <td>6.42</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.994678</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.929455</td>\n",
       "      <td>-0.368936</td>\n",
       "      <td>613.844440</td>\n",
       "      <td>263.90</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>00:40</td>\n",
       "      <td>10.208796</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0.994678</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.902886</td>\n",
       "      <td>-0.429881</td>\n",
       "      <td>477.512047</td>\n",
       "      <td>263.81</td>\n",
       "      <td>7.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228618</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:10</td>\n",
       "      <td>4.089606</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>-0.779190</td>\n",
       "      <td>-0.626788</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.984929</td>\n",
       "      <td>-0.172961</td>\n",
       "      <td>95.221129</td>\n",
       "      <td>242.16</td>\n",
       "      <td>4.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228619</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:20</td>\n",
       "      <td>4.091481</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>-0.779190</td>\n",
       "      <td>-0.626788</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.988121</td>\n",
       "      <td>-0.153676</td>\n",
       "      <td>52.295116</td>\n",
       "      <td>242.10</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228620</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:30</td>\n",
       "      <td>4.092755</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>-0.779190</td>\n",
       "      <td>-0.626788</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.989526</td>\n",
       "      <td>-0.144356</td>\n",
       "      <td>44.359521</td>\n",
       "      <td>241.94</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228621</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:40</td>\n",
       "      <td>4.096065</td>\n",
       "      <td>7.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>-0.779190</td>\n",
       "      <td>-0.626788</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.961789</td>\n",
       "      <td>-0.273791</td>\n",
       "      <td>88.027195</td>\n",
       "      <td>241.91</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228622</th>\n",
       "      <td>134</td>\n",
       "      <td>222</td>\n",
       "      <td>23:50</td>\n",
       "      <td>4.101319</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>-0.779190</td>\n",
       "      <td>-0.626788</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.917893</td>\n",
       "      <td>-0.396828</td>\n",
       "      <td>152.335686</td>\n",
       "      <td>242.09</td>\n",
       "      <td>5.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4133583 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TurbID  Day Tmstamp    WspdMA3  Wspd5    locX    locY      DayX      DayY  Weekday  \\\n",
       "720           1    6   00:00  10.235417   6.17  3000.0  6000.0  0.994678  0.103033        6   \n",
       "721           1    6   00:10  10.226273   6.17  3000.0  6000.0  0.994678  0.103033        6   \n",
       "722           1    6   00:20  10.222338   6.27  3000.0  6000.0  0.994678  0.103033        6   \n",
       "723           1    6   00:30  10.215301   6.42  3000.0  6000.0  0.994678  0.103033        6   \n",
       "724           1    6   00:40  10.208796   6.25  3000.0  6000.0  0.994678  0.103033        6   \n",
       "...         ...  ...     ...        ...    ...     ...     ...       ...       ...      ...   \n",
       "4228618     134  222   23:10   4.089606   6.90     0.0  6500.0 -0.779190 -0.626788        5   \n",
       "4228619     134  222   23:20   4.091481   6.80     0.0  6500.0 -0.779190 -0.626788        5   \n",
       "4228620     134  222   23:30   4.092755   6.75     0.0  6500.0 -0.779190 -0.626788        5   \n",
       "4228621     134  222   23:40   4.096065   7.28     0.0  6500.0 -0.779190 -0.626788        5   \n",
       "4228622     134  222   23:50   4.101319   6.86     0.0  6500.0 -0.779190 -0.626788        5   \n",
       "\n",
       "            WdirX     WdirY   Wspd_cube  Etmp_abs  Wspd  \n",
       "720     -0.958770 -0.284183  229.130355    264.20  6.12  \n",
       "721     -0.960050 -0.279829  339.998645    264.05  6.98  \n",
       "722     -0.945916 -0.324413  722.854363    263.95  8.99  \n",
       "723     -0.929455 -0.368936  613.844440    263.90  8.50  \n",
       "724     -0.902886 -0.429881  477.512047    263.81  7.82  \n",
       "...           ...       ...         ...       ...   ...  \n",
       "4228618 -0.984929 -0.172961   95.221129    242.16  4.58  \n",
       "4228619 -0.988121 -0.153676   52.295116    242.10  3.74  \n",
       "4228620 -0.989526 -0.144356   44.359521    241.94  3.54  \n",
       "4228621 -0.961789 -0.273791   88.027195    241.91  4.46  \n",
       "4228622 -0.917893 -0.396828  152.335686    242.09  5.36  \n",
       "\n",
       "[4133583 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = full_data_selected[(full_data_selected['Day']>=6)&(full_data_selected['Day']<=222)]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63741128",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd6cdb4",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:14<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Data Split\n",
      "  - Train data(X, y)     : (2540, 288, 12) (2540, 288, 12)\n",
      "  - Validation data(X, y): (23790, 288, 12) (23790, 288, 12)\n",
      "  - Test data(X)         : (134, 288, 12)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 2\n",
    "week = 144*7\n",
    "day = 144\n",
    "hour = 6\n",
    "ten_minute = 1\n",
    "val_x, val_y, train_x, train_y, test_x = make_train_val_test_data(data, in_seq_len=SEQ_LEN*144, out_seq_len=SEQ_LEN*144, stride=day, shuffle=False, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923d4223",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data(X, y): (23790, 288, 12) (23790, 288, 12)\n",
      "Validation data(X, y): (2540, 288, 12) (2540, 288, 12)\n",
      "Test data(X): (134, 288, 12)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x.reshape(-1,train_x.shape[2]))\n",
    "train_x = scaler.transform(train_x.reshape(-1,train_x.shape[2])).reshape(-1,*train_x.shape[1:])\n",
    "train_y= scaler.transform(train_y.reshape(-1,train_x.shape[2])).reshape(-1,*train_x.shape[1:])\n",
    "val_x = scaler.transform(val_x.reshape(-1,train_x.shape[2])).reshape(-1,*train_x.shape[1:])\n",
    "val_y = scaler.transform(val_y.reshape(-1,train_x.shape[2])).reshape(-1,*train_x.shape[1:])\n",
    "test_x = scaler.transform(test_x.reshape(-1,train_x.shape[2])).reshape(-1,*train_x.shape[1:])\n",
    "print(\"Train data(X, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation data(X, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test data(X):\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0763a6f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_shape = 1\n",
    "train_y = train_y[:,:,-output_shape:].reshape(-1,train_y.shape[1],output_shape)\n",
    "val_y = val_y[:,:,-output_shape:].reshape(-1,train_y.shape[1],output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889d112b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data(X, y): (23790, 288, 12) (23790, 288, 1)\n",
      "Validation data(X, y): (2540, 288, 12) (2540, 288, 1)\n",
      "Test data(X): (134, 288, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data(X, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation data(X, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test data(X):\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ddd481",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "import torch\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce7ea01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('transformer')\n",
    "from tst import Transformer\n",
    "from tst import loss\n",
    "from src.utils import compute_loss\n",
    "from src.visualization import map_plot_function, plot_values_distribution, plot_error_distribution, plot_errors_threshold, plot_visual_sample\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "# # Config\n",
    "sns.set()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c289aff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 1\n",
    "LR = 1e-4\n",
    "EPOCHS = 100\n",
    "SEQ_LEN = 2\n",
    "MARKING = 0\n",
    "\n",
    "# Model parameters\n",
    "d_model = 512 # Lattent dim\n",
    "q = 32 # Query size\n",
    "v = 32 # Value size\n",
    "h = 16 # Number of heads\n",
    "N = 16 # Number of encoder and decoder to stack\n",
    "attention_size = None # Attention window size\n",
    "dropout = 0.5 # Dropout rate\n",
    "pe = None # Positional encoding\n",
    "chunk_mode = None\n",
    "\n",
    "d_input = train_x.shape[-1] # From dataset\n",
    "d_output = train_y.shape[-1] # From dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a41ac33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CondLoss(nn.Module):\n",
    "    def __init__(self, loss_fn, marked_target_value, **kwargs):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.marked_target_value = marked_target_value\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if self.loss_fn == 'rmse':\n",
    "            self.loss_fn = torch.sqrt(nn.MSELoss())\n",
    "        elif self.loss_fn == 'mse':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        elif self.loss_fn == 'mae':\n",
    "            self.loss_fn = nn.L1Loss()\n",
    "        elif self.loss_fn == 'huber':\n",
    "            self.loss_fn = nn.HuberLoss()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def forward(self,\n",
    "                y_true: torch.Tensor,\n",
    "                y_pred: torch.Tensor) -> torch.Tensor:\n",
    "        # y_true, y_pred: [B, S, D]\n",
    "        \n",
    "        assert y_true.shape == y_pred.shape, f'Shape mismatch for output and ground truth array {y_true.shape} and {y_pred.shape}'\n",
    "\n",
    "        _, S, D = y_true.shape  # Batch, Sequence, Dim of features\n",
    "        y_true = torch.reshape(y_true, (-1, D))\n",
    "        y_pred = torch.reshape(y_pred, (-1, D))\n",
    "\n",
    "        idxs_valid = (y_true[:, -1] != self.marked_target_value)\n",
    "        y_true_valid, y_pred_valid = y_true[idxs_valid], y_pred[idxs_valid]\n",
    "        return self.loss_fn(y_pred_valid, y_true_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac3511a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = Transformer(d_input, d_model, d_output, q, v, h, N, attention_size=attention_size, dropout=dropout, chunk_mode=chunk_mode, pe=pe).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "loss_function = CondLoss('mse',0 )\n",
    "mae= nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94418d7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Transformer(\n",
       "  (layers_encoding): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (4): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (5): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (6): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (7): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (8): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (9): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (10): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (11): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (12): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (13): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (14): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (15): Encoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layers_decoding): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (4): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (5): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (6): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (7): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (8): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (9): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (10): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (11): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (12): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (13): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (14): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (15): Decoder(\n",
       "      (_selfAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_encoderDecoderAttention): MultiHeadAttention(\n",
       "        (_W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (_W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_feedForward): PositionwiseFeedForward(\n",
       "        (_linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (_linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (_layerNorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_layerNorm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (_dopout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (_embedding): Linear(in_features=12, out_features=512, bias=True)\n",
       "  (_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe62ca03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "    def __len__(self):\n",
    "            return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        text = self.text[idx]\n",
    "        sample =(text, label)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f8c9923",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train = Dataset(train_x, train_y)\n",
    "dataset_val = Dataset(val_x, val_y)\n",
    "dataset_test = Dataset(test_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e547ac8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=False\n",
    "                             )\n",
    "dataloader_val = DataLoader(dataset_val,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS\n",
    "                            )\n",
    "dataloader_test = DataLoader(dataset_test,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=NUM_WORKERS\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94eb9d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822fa85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_best = np.inf\n",
    "EPOCHS = 100\n",
    "SCORE = 9999\n",
    "# Prepare loss history\n",
    "hist_loss = np.zeros(EPOCHS)\n",
    "hist_loss_val = np.zeros(EPOCHS)\n",
    "hist_loss_mae = np.zeros(EPOCHS)\n",
    "hist_loss_mae_val = np.zeros(EPOCHS)\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    running_mae = 0\n",
    "    logs = {}\n",
    "    with tqdm(total=len(dataloader_train.dataset), desc=f\"[Epoch {idx_epoch+1:3d}/{EPOCHS}]\") as pbar:\n",
    "        for idx_batch, (x, y) in enumerate(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Propagate input\n",
    "            netout = net(x.to(device))\n",
    "\n",
    "            # Comupte loss\n",
    "            loss = loss_function(y.to(device), netout)\n",
    "            loss_mae = mae(y.to(device), netout)\n",
    "\n",
    "            # Backpropage loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_mae += loss_mae.item()\n",
    "            pbar.set_postfix({'loss_mse': running_loss/(idx_batch+1),'mae': running_mae/(idx_batch+1) })\n",
    "            pbar.update(x.shape[0])\n",
    "            \n",
    "        \n",
    "        train_loss = running_loss/len(dataloader_train)\n",
    "        train_loss_mae = running_mae/len(dataloader_train)\n",
    "        val_loss = compute_loss(net, dataloader_val, loss_function, device).item()\n",
    "        val_loss_mae = compute_loss(net, dataloader_val, mae, device).item()\n",
    "\n",
    "        pbar.set_postfix({'loss': train_loss, 'val_loss': val_loss,'mae': train_loss_mae, 'val_mae': val_loss_mae})\n",
    "\n",
    "        hist_loss[idx_epoch] = train_loss\n",
    "        hist_loss_val[idx_epoch] = val_loss\n",
    "        hist_loss_mae[idx_epoch] = train_loss_mae\n",
    "        hist_loss_mae_val[idx_epoch] = val_loss_mae\n",
    "        \n",
    "        if val_loss < val_loss_best:\n",
    "            val_loss_best = val_loss\n",
    "            model_save_path = f'saved_models/model_{val_loss}.pth'\n",
    "            torch.save(net.state_dict(), model_save_path)\n",
    "        \n",
    "    liveloss.update({ 'MSE': train_loss, 'val_MSE':val_loss, 'MAE': train_loss_mae, 'val_MAE':val_loss_mae})\n",
    "    liveloss.send()\n",
    "        \n",
    "plt.plot(hist_loss, 'o-', label='train')\n",
    "plt.plot(hist_loss_val, 'o-', label='val')\n",
    "plt.title('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"model exported to {model_save_path} with loss {val_loss_best:5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc78cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510e0fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.empty(shape=(len(dataloader_test.dataset), *train_y.shape[1:]))\n",
    "\n",
    "idx_prediction = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(dataloader_test, total=len(dataloader_test)):\n",
    "        netout = net(x.to(device)).cpu().numpy()\n",
    "        predictions[idx_prediction:idx_prediction+x.shape[0]] = netout\n",
    "        idx_prediction += x.shape[0]\n",
    "\n",
    "pred = predictions.reshape(-1,predictions.shape[2])\n",
    "pred = np.pad(pred, ((0,0),(test_x.shape[2]-output_shape,0)))\n",
    "pred_inversed = scaler.inverse_transform(pred)\n",
    "wspd = pred_inversed[:,-1]\n",
    "## Wspd^3/Etmp_abs*(A*rou)*C\n",
    "target = np.clip(wspd**3/300*807*0.2,0,1500)\n",
    "pred_inversed[:,-1] = target\n",
    "submission['Patv'] = pred_inversed[:,-1]\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "submission.to_csv(f\"output/prediction_transformer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full",
   "language": "python",
   "name": "full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}