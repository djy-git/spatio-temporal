{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00204c1d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8198c90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('analysis-tools')\n",
    "from analysis_tools.common.utils import *\n",
    "from analysis_tools import utils, random, eda\n",
    "\n",
    "import tensorflow as tf\n",
    "utils.set_memory_growth()\n",
    "random.set_random_seed_tf()\n",
    "\n",
    "exp_name = \"GRU\"\n",
    "class PATH:\n",
    "    root   = abspath('.')\n",
    "    input  = join(root, 'data')\n",
    "    output = join(root, 'output')\n",
    "    ckpt   = join(root, 'ckpt', exp_name)\n",
    "    eda    = join(root, 'eda', exp_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc277390",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Load Dataset\n",
    "- TurbID \\\n",
    "Wind turbine ID, 발전기 ID (1~134)\n",
    "- Day \\\n",
    "Day of the record, 날짜 (1~222)\n",
    "- Tmstamp(HH:MM) \\\n",
    "Created time of the record, 시간 (00:00 ~ 23:50)\n",
    "- Wspd(m/s) \\\n",
    "The wind speed recorded by the anemometer, 풍속\n",
    "- Wdir(°) \\\n",
    "wind direction, 터빈이 바라보는 각도와 실제 바람 방향 각도 차이\n",
    "- Etmp(℃) \\\n",
    "Temperature of the surounding environment, 외부 온도\n",
    "- Itmp(℃) \\\n",
    "Temperature inside the turbine nacelle, 터빈 내부 온도\n",
    "- Ndir(°) \\\n",
    "Nacelle direction, i.e., the yaw angle of the nacelle, 터빈이 바라보는방향 각도\n",
    "- Pab(°) \\\n",
    "Pitch angle of blade,터빈 당 3개의 날이 있으며 각각의 각도가 다름\n",
    "- Prtv(kW) \\\n",
    "Reactive power, 무효전력 : 에너지원을 필요로 하지 않는 전력\n",
    "- **Patv(kW) -> Target** \\\n",
    "Active power, 유효전력 : 실제로 터빈을 돌리는 일을 하는 전력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38ea9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_raw  = pd.read_csv(join(PATH.input, 'train_data.csv'))\n",
    "\n",
    "test_data_raw = data_raw.query(\"Day >= 244\")\n",
    "data_raw = data_raw.query(\"Day < 244\")\n",
    "\n",
    "cols_raw = data_raw.columns\n",
    "target = 'Patv'\n",
    "data_raw.info()\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a256d88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. EDA\n",
    "1. Features vs Target(Patv)\n",
    "    - TurbID, Day, Etmp, Itmp는 뚜렷한 관계가 보이지 않음\n",
    "    - corr(Wspd, Patv) >> 0\n",
    "        - 특정 값(1) 이하이면, Patv=0\n",
    "    - Wdir: abs(Wdir)이 특정값(10) 이상이면, Patv=0\n",
    "    - Ndir: 특정값(0, 180, 360)에서 Patv 높음\n",
    "    - Pab1,2,3: 특정 값(20) 이상이면, Patv=0\n",
    "        - 대략 0.03을 기준으로 층이 보인다.\n",
    "    - Prtv: Patv와 선형적인 관계가 보인다.\n",
    "        - Prtv<0의 경우와 Prtv>0의 경우, Patv와 선형적인 관계를 가지고 있기 때문에 Prtv_neg, Prtv_pos 로 분리하면 더 모델이 잘 이해할 수 있을 것 같다.\n",
    "2. 전체적으로 이상치가 많다.\n",
    "    - 특히, Patv=0 인 이상치들이 많아 보인다.\n",
    "    - 이상치를 제거하면 더 관계를 잘 파악할 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c29f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_features(data_raw)\n",
    "# eda.plot_features_target(data_raw, target, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e5ecf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_ts_features(data_raw.query(\"TurbID == 1\").drop(columns=['TurbID']), title='TurbID=1', figsize=(30, 8))\n",
    "# eda.plot_ts_features(data_raw.query(\"TurbID == 2\").drop(columns=['TurbID']), title='TurbID=2', figsize=(30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabcf7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "tmp = data_raw.sample(100000)\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930e1b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph = figure()\n",
    "graph.scatter(tmp['Wdir'], tmp[target], alpha=0.1)\n",
    "show(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17751ced",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp[tmp['Wdir'].abs() < 10][target].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9c201",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp[tmp['Wdir'].abs() > 10][target].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc3bbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# graph = figure()\n",
    "# graph.scatter(tmp['Ndir'], tmp[target], alpha=0.1)\n",
    "# show(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5fe91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# graph = figure()\n",
    "# graph.scatter(tmp['Pab1'], tmp[target], alpha=0.1)\n",
    "# show(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d33e58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a = tmp['Pab1'].value_counts().sort_index()\n",
    "# a[a.index > 0].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c11c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# graph = figure()\n",
    "# graph.scatter(tmp['Wspd'], tmp[target], alpha=0.1)\n",
    "# show(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bc155",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# eda.plot_pair(data_raw, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d1195",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## - Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d175733",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# def plot_acfs(data, cols=None, lags=144*30+1):\n",
    "#     if cols is None:\n",
    "#         cols = data.columns\n",
    "#     acfs = {}\n",
    "#     fig, axes = plt.subplots(len(cols), figsize=(40, len(cols)*2))\n",
    "#     for ax, col in zip(axes.flat, cols):\n",
    "#         plot_acf(data[col], lags=lags, missing='drop', auto_ylims=True, ax=ax, title=col)\n",
    "#         xticks      = np.arange(0, lags, 144)\n",
    "#         xticklabels = [f\"{xtick//144} {'day' if xtick<=144 else 'days'}\" for xtick in xticks]\n",
    "#         ax.set_xticks(xticks, xticklabels)\n",
    "#         ax.set_ylim([-1, 1])\n",
    "#         ax.set_xlim([-1, lags+1])\n",
    "\n",
    "#         _, line2 = ax.lines\n",
    "#         acf = line2.get_ydata()[144*2:]\n",
    "#         sorted_acf = np.array(sorted(enumerate(acf), key=lambda x: abs(x[1]), reverse=True), dtype='float32')\n",
    "#         sorted_acf[:, 0] += 144*2\n",
    "#         acfs[col] = sorted_acf\n",
    "        \n",
    "#         acf, _ = acfs[col][0]\n",
    "#         ax.axvline(acf, color='k')\n",
    "#     for ax in axes.flat[:-1]:\n",
    "#         ax.set_xticklabels([])\n",
    "    \n",
    "#     return acfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf103fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# acfs = plot_acfs(data_proc.query(\"TurbID == 10\"), cols=['Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir', 'Pab1', 'Prtv', 'Patv'])\n",
    "# acfs = plot_acfs(data_proc.query(\"TurbID == 20\"), cols=['Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir', 'Pab1', 'Prtv', 'Patv'])\n",
    "# acfs = plot_acfs(data_proc.query(\"TurbID == 30\"), cols=['Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir', 'Pab1', 'Prtv', 'Patv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57501cb6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b2cc6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## - Missing values\n",
    "1. 전체 4283712개 row중 48605개 row(1.1%)가 nan값을 가지고 있음\n",
    "2. Nan값의 특징\n",
    "    - 특정 TurbID에 많음 (126)\n",
    "    - 특정 Day에 많음 (66, 67, 201)\n",
    "    - 특정 Tmstamp에 많음 (00:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee500a82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_missing_value(data_raw, figsize=(30, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4692260",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_ts_features(data_raw.query(\"TurbID == 126\").drop(columns=['TurbID']))\n",
    "# eda.plot_ts_features(data_raw.query(\"TurbID == 127\").drop(columns=['TurbID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6aced",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_notnull = data[data_raw.notnull()]\n",
    "# data_null    = data[data_raw.isna().any(1)]\n",
    "\n",
    "# eda.plot_features(data_raw, title=\"Full data\")\n",
    "# eda.plot_features(data_notnull, title=\"Not null data\")\n",
    "# eda.plot_features(data_null, title=\"Null data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaf98a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SAVE = True\n",
    "def save_data(data, name):\n",
    "    if SAVE:\n",
    "        data.to_csv(join(PATH.input, f'{name}.csv'), index=False)\n",
    "\n",
    "def check_nan(data, name):\n",
    "    \"\"\"Print number of data and nan rows\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        Input data\n",
    "    name : str\n",
    "        Name to identify data\n",
    "    \"\"\"\n",
    "    print(\"* Data name:\", name)\n",
    "    print(\"  - Number of rows:\", len(data))\n",
    "    print(\"  - Number of nan rows:\", sum(data.isna().sum(axis='columns') > 0))\n",
    "   \n",
    "def compare_data(data1, data2, cols=None):\n",
    "    if cols is None:\n",
    "        cols = list(data1)\n",
    "    eda.plot_features(data1[cols], data2[cols], figsize=(30, 8))\n",
    "    eda.plot_pair(data1[cols], data2[cols], sample=1000, figsize=(15, 15))\n",
    "    eda.plot_corr(data1[cols].corr(), data2[cols].corr(), figsize=(10, 10))\n",
    "    check_nan(data1, \"data1\")\n",
    "    check_nan(data2, \"data2\")\n",
    "    \n",
    "def get_efficiency(data):\n",
    "    # Patv ≈ Efficiency * 0.5 * Wspd_cube * A * density\n",
    "    # Efficiency < 0.6\n",
    "    vals      = data['Wspd'].unique()\n",
    "    min_val   = vals[vals > 0].min()\n",
    "    Wspd      = np.maximum(data['Wspd'], min_val)\n",
    "    Patv      = np.maximum(data['Patv'], 0)\n",
    "\n",
    "    Wspd_cube = Wspd**3\n",
    "    A         = np.pi * 82**2\n",
    "    density   = 1.225\n",
    "    return (Patv*1000) / (0.5*Wspd_cube * A * density)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15e9b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## - Handle Errors\n",
    "1. Patv값을 신뢰할 수 없는(Abnormal=1) row의 개수가 975144개(22.8%)로 적지 않다.\n",
    "    - cond1: Patv <= 0 & Wspd > 2.5: 287362개(6.7%)\n",
    "        - Patv <= 0 인 것이 딱히 문제되지는 않을 것 같다.\n",
    "    - cond2: Pab1 > 89 | Pab2 > 89 | Pab3 > 89: 834589개(19.5%)\n",
    "        - Pab의 최댓값이 99.98이고 20 이상의 값은 전부 0이기 때문에 0으로 처리\n",
    "    - cond3: (Wdir < -180 | Wdir > 180) | (Ndir < -720 | Ndir > 720): 127(0.0%)\n",
    "        - Pab와 마찬가지로 값이 특정값 이상이거나 이하인 경우 0인 경향이 크므로 0으로 처리\n",
    "    - cond4: Patv is null: 48605개(1.1%)\n",
    "        - 나중에 Patv를 modeling해서 채우자\n",
    "2. Patv값을 신뢰할 수 없는 경우, 대부분의 값(whislo ~ whishi)의 Patv=0\n",
    "3. Wdir이 180도를 넘어가거나 Ndir이 720도를 넘어가는 경우(cond3), 여전히 대부분의 값(whislo ~ whishi)은 Patv=0이나, 큰 Patv값이 나오고 Patv의 변화가 가장 컸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11effd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_datetime(data, start_date=datetime.datetime(2020, 12, 31)):\n",
    "    data = copy(data)\n",
    "    data['Day_tmp'] = (start_date + pd.to_timedelta(data['Day'], unit='day')).astype('string')\n",
    "    data['Date'] = pd.to_datetime(data.apply(lambda row: f\"{row['Day_tmp']} {row['Tmstamp']}:00\", axis='columns'))\n",
    "    for turbID in data['TurbID'].unique():\n",
    "        d = data[data['TurbID'] == turbID]\n",
    "        assert d['Date'].is_monotonic_increasing, \"Data should be sorted\"\n",
    "        data.loc[data['TurbID'] == turbID, 'Time'] = 1 + np.arange(len(d))\n",
    "    data.drop(columns=['Day_tmp'], inplace=True)\n",
    "    data['Time'] = data['Time'].astype('int32')\n",
    "    check_nan(data, \"Generate datetime\")\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "def get_idxs_abnormal(data):\n",
    "    cond1 = (data['Patv'] <= 0) & (data['Wspd'] > 2.5)\n",
    "    cond2 = (data['Pab1'] > 89) | (data['Pab2'] > 89) | (data['Pab3'] > 89)\n",
    "    cond3 = (data['Wdir'] < -180) | (data['Wdir'] > 180) | (data['Ndir'] < -720) | (data['Ndir'] > 720)\n",
    "    cond4 = data['Patv'].isnull()\n",
    "    cond  = cond1 | cond2 | cond3 | cond4\n",
    "    return np.where(cond)[0], np.where(cond1)[0], np.where(cond2)[0], np.where(cond3)[0], np.where(cond4)[0]\n",
    "\n",
    "def mark_abnormal_Patv(data):\n",
    "    data = copy(data)\n",
    "    data['Abnormal'] = 0\n",
    "    data[[f'Abnormal_{i}' for i in [1, 2, 3, 4]]] = 0\n",
    "    idxs_full, idxs_1, idxs_2, idxs_3, idxs_4 = get_idxs_abnormal(data)\n",
    "    for num, i in enumerate([idxs_1, idxs_2, idxs_3, idxs_4], start=1):\n",
    "        data.loc[data.iloc[i].index, f'Abnormal_{num}'] = 1\n",
    "    data.loc[data.iloc[idxs_full].index, 'Abnormal'] = 1\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(5, figsize=(40, 8))\n",
    "    for ax, col in zip(axes, ['Abnormal'] + [f'Abnormal_{i}' for i in [1, 2, 3, 4]]):\n",
    "        sns.boxplot(x='Patv', y=col, data=data, orient='h', ax=ax);\n",
    "    for ax in axes[:-1]:\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xlabel(None)        \n",
    "\n",
    "    # Adjust Patv\n",
    "    data['Patv'] = data['Patv'].where(data['Abnormal_2'] == 0, 0)\n",
    "    data['Patv'] = data['Patv'].where(data['Abnormal_3'] == 0, 0)\n",
    "\n",
    "    data['Ndir'] = np.clip(data['Ndir'], -720, 720)\n",
    "    data['Wdir'] = np.clip(data['Wdir'], -180, 180)\n",
    "    for i in [1, 2, 3]:\n",
    "        data[f'Pab{i}'] = np.clip(data[f'Pab{i}'], 0, 89)\n",
    "    \n",
    "    check_nan(data, \"Mark anomaly Patv\")\n",
    "    return data\n",
    "\n",
    "def manual_handling(data):\n",
    "    def mark(data, col, min_val, max_val):\n",
    "        data.loc[(data[col] < min_val) | (max_val < data[col]), col] = None\n",
    "\n",
    "    data = copy(data)\n",
    "    mark(data, 'Etmp', -20, 80)\n",
    "    mark(data, 'Itmp', -10, 65)\n",
    "    \n",
    "    data.loc[(data['TurbID'] == 2) & (7533 <= data['Time']) & (data['Time'] <= 7899), 'Etmp']   = None\n",
    "    data.loc[(data['TurbID'] == 2) & (24408 <= data['Time']) & (data['Time'] <= 24414), 'Etmp'] = None\n",
    "    data.loc[(data['TurbID'] == 2) & (26562 <= data['Time']) & (data['Time'] <= 26575), 'Etmp'] = None\n",
    "    \n",
    "    check_nan(data, \"Manual handling\")\n",
    "    return data\n",
    "\n",
    "def outlier_handling(data, columns):\n",
    "    @delayed\n",
    "    def task(data_tid, window_size):\n",
    "        for day in data_tid['Day'].unique():\n",
    "            for column in columns:\n",
    "                temp = data_tid.loc[(day <= data_tid['Day']) & (data_tid['Day'] <= day + window_size - 1), [column]]\n",
    "                temp[f'{column}_diff'] = temp[column].diff(1)\n",
    "                for col in (column, f'{column}_diff'):\n",
    "                    stats = boxplot_stats(temp.dropna()[col])[0]\n",
    "                    temp[col].where((stats['whislo'] <= temp[col]) & (temp[col] <= stats['whishi']), np.nan, inplace=True)\n",
    "                temp.drop(columns=[f'{column}_diff'], inplace=True)\n",
    "                data_tid.loc[temp.index, [column]] = temp.values\n",
    "        return data_tid\n",
    "    \n",
    "    data = data.copy()\n",
    "    tasks = [task(data[data['TurbID'] == turbID], window_size=2) for turbID in data['TurbID'].unique()]\n",
    "    with ProgressBar():\n",
    "        data_tids = compute(*tasks, scheduler='processes')\n",
    "    \n",
    "    for data_tid in tqdm(data_tids):\n",
    "        data.loc[data_tid.index] = data_tid.values\n",
    "    \n",
    "    check_nan(data, \"Outlier handling\")\n",
    "    return data\n",
    "\n",
    "def find_neighbor(data, col, my_TurbID, plot=False):\n",
    "    df = pd.DataFrame()\n",
    "    for turbID in data['TurbID'].unique():\n",
    "        df[turbID] = data[data['TurbID'] == turbID][col].values\n",
    "    res = df - df[[my_TurbID]].values\n",
    "    mae = res.abs().mean()\n",
    "    rst = mae.drop(my_TurbID).idxmin()\n",
    "    if plot:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, figsize=(40, 5))\n",
    "        mae.plot.bar(ax=ax1)\n",
    "        df[[my_TurbID, rst]].plot(ax=ax2)\n",
    "    return rst\n",
    "\n",
    "def get_dists_loc(input_dir):\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "    loc = pd.read_csv(join(input_dir, 'turb_location.csv')).set_index('TurbID')\n",
    "    tids = loc.index\n",
    "    dists_loc = np.full((len(loc)+1, len(loc)+1), np.inf)\n",
    "    for i1 in range(len(loc)-1):\n",
    "        for i2 in range(i1+1, len(loc)):\n",
    "            t1, t2 = tids[i1], tids[i2]\n",
    "            dists_loc[i1, i2] = euclidean_distances([loc.loc[t1].values], [loc.loc[t2].values])\n",
    "    return dists_loc\n",
    "\n",
    "def greedy_imputing(data, tids=None, k=10, verbose=False):\n",
    "    def greedy_imputing_tid(data, tid_target, dists_loc, k, verbose):\n",
    "        def affinity(tid):\n",
    "            d        = data[data['TurbID'] == tid][cols].values\n",
    "            d_target = data[data['TurbID'] == tid_target][cols].values\n",
    "            diff = np.abs(d - d_target)\n",
    "            return diff[~np.isnan(diff)].mean()\n",
    "        \n",
    "        cols       = ['Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir', 'Pab1', 'Pab2', 'Pab3', 'Prtv', 'Patv']\n",
    "        neighbors  = [tid for tid, dist_loc in sorted(enumerate(dists_loc[tid_target], start=1), key=lambda x: x[1])[:k]]\n",
    "        n_nan_prev = np.sum(data.loc[data['TurbID'] == tid_target, cols].isna().values)\n",
    "\n",
    "        for tid in sorted(neighbors, key=affinity):\n",
    "            a = data.loc[data['TurbID'] == tid_target, cols].values\n",
    "            b = data.loc[data['TurbID'] == tid, cols].values\n",
    "            data.loc[data['TurbID'] == tid_target, cols] = np.where(np.isnan(a) & (~np.isnan(b)), b, a)\n",
    "\n",
    "        n_nan_after = np.sum(data[data['TurbID'] == tid_target].isna().values)\n",
    "        if verbose and n_nan_after < n_nan_prev:\n",
    "            print(f\"[TurbID {tid_target}] Number of nan: {n_nan_prev} -> {n_nan_after}\")\n",
    "        return n_nan_after < n_nan_prev\n",
    "\n",
    "    data = copy(data)\n",
    "    dists_loc = get_dists_loc(PATH.input)\n",
    "\n",
    "    if tids is None:\n",
    "        tids = data['TurbID'].unique()\n",
    "\n",
    "    while True:\n",
    "        rsts = {}\n",
    "        for tid in tqdm(tids):\n",
    "            rsts[tid] = greedy_imputing_tid(data, tid, dists_loc, k, verbose)\n",
    "        if not any(rsts.values()) or (k == 134):\n",
    "            break\n",
    "\n",
    "    check_nan(data, \"Greedy imputing\")\n",
    "    return data\n",
    "\n",
    "def impute_data(data, threshold=6 * 12, drop=False):\n",
    "    data = copy(data)\n",
    "    data_imp = pd.DataFrame()\n",
    "    for turbID in tqdm(data['TurbID'].unique()):\n",
    "        data_tid = data[data['TurbID'] == turbID]\n",
    "        idxs = (data_tid.isna().sum(axis='columns') > 0)\n",
    "        idxs_nan = idxs[idxs].index\n",
    "        idxs_removed = []\n",
    "\n",
    "        if len(idxs_nan) > 0:\n",
    "            s, e = 0, 1\n",
    "            while e < len(idxs_nan):\n",
    "                cur = idxs_nan[s:e]\n",
    "                if idxs_nan[e] == cur[-1] + 1:\n",
    "                    e += 1\n",
    "                else:\n",
    "                    if len(cur) >= threshold:\n",
    "                        idxs_removed += list(cur)\n",
    "                    s = e\n",
    "                    e = s + 1\n",
    "            else:\n",
    "                if len(cur) >= threshold:\n",
    "                    idxs_removed += list(cur)\n",
    "\n",
    "        data_tid_drop = data_tid.drop(idxs_removed)\n",
    "        tmp = data.isna().sum()\n",
    "        cols_nan = tmp[tmp > 0].index\n",
    "        data_tid_drop[cols_nan] = data_tid_drop[cols_nan].interpolate().bfill().ffill()\n",
    "        if drop:\n",
    "            data_tid_imp = data_tid_drop\n",
    "        else:\n",
    "            data_tid_imp = data_tid\n",
    "            data_tid_imp.loc[data_tid_drop.index] = data_tid_drop.values\n",
    "        data_imp = data_imp.append(data_tid_imp)\n",
    "\n",
    "    check_nan(data_imp, \"Imputing\")\n",
    "    return data_imp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff374a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data1 = generate_datetime(data_raw)\n",
    "save_data(data1, \"data1\")\n",
    "\n",
    "data2 = mark_abnormal_Patv(data1)\n",
    "save_data(data2, \"data2\")\n",
    "\n",
    "data3 = manual_handling(data2)\n",
    "save_data(data3, \"data3\")\n",
    "\n",
    "data4 = outlier_handling(data3, ['Etmp', 'Itmp'])\n",
    "save_data(data4, 'data4')\n",
    "\n",
    "data5 = greedy_imputing(data4, tids=[2, 7, 8, 9, 13, 18, 24, 25, 26, 29, 30, 33, 36, 38, 40, 41, 50, 54, 60, 61, 65, 66, 67, 68, 79, 80, 82, 87, 88, 105, 118, 121, 122, 126, 129], k=20)\n",
    "save_data(data5, 'data5')\n",
    "\n",
    "data6 = impute_data(data5)\n",
    "save_data(data6, 'data6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89ad3e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tmp = data3\n",
    "# eda.plot_two_features(tmp, 'Wspd', 'Patv', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deb4e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_features_target(data[cols_raw], target, alpha=0.01)\n",
    "# eda.plot_features_target(data2[cols_raw], target, alpha=0.01)\n",
    "# eda.plot_features_target(data3, target, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfc983",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cols = cols_raw.drop(['TurbID', 'Pab2', 'Pab3'])\n",
    "\n",
    "# eda.plot_ts_features(data3.query(\"TurbID == 1\")[cols])\n",
    "# eda.plot_ts_features(data3.query(\"TurbID == 2\")[cols])\n",
    "# eda.plot_ts_features(data3.query(\"TurbID == 67\")[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23c490",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_missing_value(data4[cols_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cba70b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cols = cols_raw.drop(['TurbID', 'Pab2', 'Pab3'])\n",
    "\n",
    "# eda.plot_ts_features(data4.query(\"TurbID == 1\")[cols])\n",
    "# eda.plot_ts_features(data4.query(\"TurbID == 2\")[cols])\n",
    "# eda.plot_ts_features(data4.query(\"TurbID == 67\")[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45d206",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_features_target(data4[cols_raw], target, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc74ebc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data6 = pd.read_csv(join(PATH.input, 'data6.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102d603",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# eda.plot_missing_value(data6)\n",
    "# eda.plot_features_target(data6, target, alpha=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e75760",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tid in data6['TurbID'].unique():\n",
    "    data = data6.query(f\"TurbID == {tid}\")\n",
    "    data_nan = data[data.isna().any(1)]\n",
    "    \n",
    "    print(\"TurbID:\", tid)\n",
    "    display(data_nan.groupby(\"Day\").count()['Time'].to_frame().T.style.background_gradient(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c790fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir', 'Pab1', 'Prtv', 'Patv']\n",
    "for tid in data6['TurbID'].unique():\n",
    "    data = data6.query(f\"TurbID == {tid}\")[cols]\n",
    "    eda.plot_ts_features(data, title=f\"TurbID={tid}\", figsize=(40, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cd3cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_imp = data6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86adca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c5347",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    data = copy(data)\n",
    "    \n",
    "    # 1. EDA features\n",
    "    data['Wspd_active']  = data['Wspd'] - 1\n",
    "    data['Wspd_extreme'] = data['Wspd'] < 1\n",
    "    data['Wspd_comb']    = data['Wspd_extreme'] * data['Wspd_active']\n",
    "    \n",
    "    data['Wdir_active']  = data['Wdir'].abs() - 10\n",
    "    data['Wdir_extreme'] = data['Wdir'].abs() > 10\n",
    "    data['Wdir_comb']    = data['Wdir_extreme'] * data['Wdir_active']\n",
    "    \n",
    "    Ndir_rad             = data['Ndir']\n",
    "    data['Ndir_180']     = data['Ndir'].map(lambda x: np.min([abs(x-0), abs(x-180), abs(x-360)]))  # -180은 안 됨\n",
    "    data['Ndir_extreme'] = data['Ndir_180'] > 90\n",
    "    data['Ndir_comb']    = data['Ndir_extreme'] * data['Ndir_180']\n",
    "\n",
    "    data['Pab']          = (data['Pab1'] + data['Pab2'] + data['Pab3'])/3\n",
    "    data['Pab_extreme1'] = data['Pab'] < 0.03\n",
    "    data['Pab_extreme2'] = data['Pab'] > 20\n",
    "\n",
    "    data['Prtv_pos']     = data['Prtv'] > 0\n",
    "    data['Prtv_abs']     = data['Prtv'].abs()\n",
    "    data['Prtv_comb']    = data['Prtv_pos'] * data['Prtv_abs']\n",
    "    \n",
    "    \n",
    "    # 2. Time\n",
    "    DAY = 6*24  # 10minute * 6 * 24hour\n",
    "    Time_in_day     = data['Time'] * (2*np.pi) / DAY\n",
    "    data['Day_cos'] = np.cos(Time_in_day)\n",
    "    data['Day_sin'] = np.sin(Time_in_day)\n",
    "\n",
    "    YEAR = 365*DAY\n",
    "    Time_in_year     = data['Time'] * (2*np.pi) / YEAR\n",
    "    data['Year_cos'] = np.cos(Time_in_year)\n",
    "    data['Year_sin'] = np.sin(Time_in_year)\n",
    "    data['Weekday']  = data['Day'] % 7\n",
    "    \n",
    "    # 3. Direction\n",
    "    Wdir_rad         = np.radians(data['Wdir'])\n",
    "    data['Wdir_cos'] = np.cos(Wdir_rad)\n",
    "    data['Wdir_sin'] = np.sin(Wdir_rad)\n",
    "    data['Wspd_cos'] = data['Wspd_active'] * np.cos(Wdir_rad)\n",
    "    data['Wspd_sin'] = data['Wspd_active'] * np.sin(Wdir_rad)\n",
    "    \n",
    "    data['Ndir_cos'] = np.cos(Ndir_rad)\n",
    "    data['Ndir_sin'] = np.sin(Ndir_rad)\n",
    "    \n",
    "    # 4. RPM\n",
    "    ALPHA = 40\n",
    "    Pab_rad     = np.radians(data['Pab']+ALPHA)\n",
    "    data['TSR'] = 1 / np.tan(Pab_rad)\n",
    "    data['RPM'] = data['Wspd_active'] * data['TSR']\n",
    "    \n",
    "    # 5. Patv\n",
    "    data['Wspd_cube'] = data['Wspd_active']**3\n",
    "    min_val = data[data['Patv'] > 0]['Patv'].min()\n",
    "    data['Patv_pos']  = np.maximum(data['Patv'], min_val)\n",
    "    data['Patan_abs'] = np.arctan(data['Prtv_abs'] / data['Patv_pos'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def feature_engineering_lag(data, cols, lags):\n",
    "    data = copy(data)\n",
    "    for turbID in tqdm(data['TurbID'].unique()):\n",
    "        data_tid = data[data['TurbID'] == turbID]\n",
    "        for lag in lags:\n",
    "            data.loc[data_tid.index, [f'{col}_LAG{lag}' for col in cols]] = data_tid[cols].shift(144*lag).values\n",
    "#         for lag in [3]:\n",
    "#             data.loc[data_tid.index, [f'{col}_MA{lag}' for col in cols]]  = data_tid[cols].rolling(144*lag).mean().values\n",
    "    data = data[data['Day'] > max(lags)]\n",
    "    check_nan(data, \"Feature engineering2\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58c8b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_fe = feature_engineering(data_imp)\n",
    "# save_data(data_fe, \"data_fe\")\n",
    "data_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f3a1a",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eda.plot_features_target(data_fe, target, alpha=0.01, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05225a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_fe_bfill = copy(data_fe)\n",
    "data_nan = data_fe_bfill[data_fe_bfill.isna().any(1)]\n",
    "idxs_nan = data_nan.index\n",
    "\n",
    "tmp = data_nan.isna().sum() > 0\n",
    "cols = tmp[tmp].index\n",
    "\n",
    "data_fe_bfill.loc[data_fe_bfill['Abnormal'] == 1, cols] = None\n",
    "data_fe_bfill.bfill(inplace=True)\n",
    "\n",
    "data_fe_bfill.loc[idxs_nan, cols] = data_nan[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a712ebd",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eda.plot_features_target(data_fe_bfill, target, alpha=0.01, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a786e37",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Feature Selection\n",
    "1. 전부 사용하면 overfitting이 되기 십상이다.\n",
    "2. 여러 번의 피드백을 통해 feature를 선택하는 것이 바람직하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad665911",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cols_sel = ['TurbID', 'Day', 'Time']\n",
    "# cols_sel += ['Wspd', 'Wspd_extreme', 'Wspd_cube',\n",
    "#              'Wdir_extreme',\n",
    "#              'Ndir_180', 'Ndir_extreme',\n",
    "#              'Pab_extreme1', 'Pab_extreme2',\n",
    "#              'Prtv', 'Prtv_pos', 'Patan_abs',\n",
    "#              'Etmp', 'Itmp',\n",
    "#              'RPM']\n",
    "# cols_sel += [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f9adf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_sel = ['TurbID', 'Day', 'Time']\n",
    "cols_sel += ['Etmp', 'Itmp', 'Prtv', \n",
    "             'Wspd', 'Wspd_extreme', 'Wspd_comb', \n",
    "             'Wdir_active', 'Wdir_extreme', 'Wdir_comb', \n",
    "             'Ndir_cos', 'Ndir_sin', 'Ndir_extreme', 'Ndir_comb', \n",
    "             'Pab', 'Pab_extreme1', 'Pab_extreme2', \n",
    "             'Prtv_pos', 'Prtv_abs', 'Prtv_comb', \n",
    "             'Wdir_cos', 'Wdir_sin', 'Wspd_cos', 'Wspd_sin', \n",
    "             'RPM', 'Wspd_cube', \n",
    "             'Patan_abs']\n",
    "cols_sel += ['Day_cos', 'Day_sin', 'Year_cos', 'Year_sin']\n",
    "cols_sel += [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca920498",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_sel = data_fe[cols_sel]\n",
    "# data_sel = data_fe_bfill[cols_sel]\n",
    "data_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394dd87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_sel = feature_engineering_lag(data_sel, data_sel.columns.drop([\"TurbID\", 'Day', 'Time']), lags=[5])\n",
    "data_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395c8aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695c4be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_in = data_sel.columns.drop(['Day', 'Time'])\n",
    "idx_cols_in = [i for i, col in enumerate(data_sel.columns) if col in cols_in]\n",
    "\n",
    "cols_out = ['Wspd', 'Patv']  # ['Etmp', 'Itmp', 'Prtv', 'Wspd_active', 'Wdir', 'Ndir', 'Pab', 'Patv']\n",
    "idx_cols_out = [i for i, col in enumerate(data_sel.columns) if col in cols_out]\n",
    "\n",
    "print(cols_in)\n",
    "print(cols_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b6993",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_shape(*args):\n",
    "    print(\"* Shape of dataset\")\n",
    "    for arg in args:\n",
    "        print(f\"  - {arg.shape}\")\n",
    "\n",
    "def scale(data, scaler):\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data, dtype='float32')\n",
    "    if len(data.shape) == 3:\n",
    "        return scaler.transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "    elif len(data.shape) == 2:\n",
    "        return scaler.transform(data)\n",
    "    else:\n",
    "        raise ValueError(\"len(data.shape) should be 2 or 3\")\n",
    "        \n",
    "def inverse_scale(data, scaler):\n",
    "    if len(data.shape) == 3:\n",
    "        return scaler.inverse_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
    "    elif len(data.shape) == 2:\n",
    "        return scaler.inverse_transform(data)\n",
    "    else:\n",
    "        raise ValueError(\"len(data.shape) should be 2 or 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071bd6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Data split 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f7715",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from analysis_tools import preprocessing\n",
    "\n",
    "\n",
    "def sliding_window(df, in_seq_len, out_seq_len, stride):\n",
    "    if len(df) == in_seq_len:\n",
    "        # test set\n",
    "        return [df.values], None\n",
    "    elif len(df) > in_seq_len:\n",
    "        # training, validation set\n",
    "        df_in  = df.iloc[:-out_seq_len]\n",
    "        df_out = df.iloc[in_seq_len:]\n",
    "    else:\n",
    "        raise ValueError(\"len(df) is too short\")\n",
    "\n",
    "    ins, outs = [], []\n",
    "    for i in range(0, len(df) - in_seq_len - out_seq_len + 1, stride):\n",
    "        in_sel  = df_in.iloc[i:i + in_seq_len]\n",
    "        ins.append(in_sel.values)\n",
    "        out_sel = df_out.iloc[i:i + out_seq_len]\n",
    "        outs.append(out_sel.values)\n",
    "    return np.array(ins), np.array(outs)\n",
    "\n",
    "\n",
    "def split_dataframe(df, in_seq_len, out_seq_len, stride, val_size):\n",
    "    df = df.astype('float32')\n",
    "\n",
    "    # 1. Sliding windows 생성\n",
    "    windows_in, windows_out = [], []\n",
    "    for tid in df['TurbID'].unique():\n",
    "        window_in, window_out = sliding_window(df[df['TurbID'] == tid], in_seq_len, out_seq_len, stride)\n",
    "        windows_in.append(window_in);  windows_out.append(window_out)\n",
    "    windows_in, windows_out = np.array(windows_in), np.array(windows_out)\n",
    "\n",
    "    T, B, S, D = windows_in.shape\n",
    "    if windows_out.ndim == 1:  # test set\n",
    "        pass\n",
    "    else:\n",
    "        T, B_, S_, D = windows_out.shape\n",
    "\n",
    "    if B == 1:  # if there is one sequence, then test set\n",
    "        return windows_in.reshape(T, S, D)\n",
    "    else:  # training, validation set\n",
    "        idxs_in,  idxs_out  = train_test_split(range(len(windows_in[1])), test_size=val_size)\n",
    "        train_in, train_out = [], []\n",
    "        val_in,   val_out   = [], []\n",
    "\n",
    "        for window_in, window_out in zip(windows_in, windows_out):\n",
    "            for idxs, ins, outs in zip([idxs_in, idxs_out], [train_in, val_in], [train_out, val_out]):\n",
    "                for idx in idxs:\n",
    "                    in_sel  = window_in[idx]\n",
    "                    out_sel = window_out[idx]\n",
    "                    if (np.isnan(in_sel).any()) or (np.isnan(out_sel).any()):\n",
    "                        continue\n",
    "                    ins.append(in_sel)\n",
    "                    outs.append(out_sel)\n",
    "        train_in  = np.array(train_in)\n",
    "        train_out = np.array(train_out)\n",
    "        val_in    = np.array(val_in)\n",
    "        val_out   = np.array(val_out)\n",
    "\n",
    "        return train_in, val_in, train_out, val_out\n",
    "    \n",
    "    \n",
    "input_days  = 2\n",
    "IN_SEQ_LEN  = input_days * 144\n",
    "OUT_SEQ_LEN = 2 * 144\n",
    "STRIDE      = 2 * 144\n",
    "VAL_SIZE    = 0.1\n",
    "\n",
    "n_tids          = data_sel['TurbID'].nunique()\n",
    "max_day         = data_sel['Day'].max()\n",
    "train_full_data = data_sel[data_sel['Day'] <= max_day-input_days]\n",
    "test_data       = data_sel[max_day-input_days < data_sel['Day']]\n",
    "\n",
    "train_in, val_in, train_out, val_out = split_dataframe(train_full_data, IN_SEQ_LEN, OUT_SEQ_LEN, STRIDE, VAL_SIZE)\n",
    "test_in  = split_dataframe(test_data, IN_SEQ_LEN, OUT_SEQ_LEN, STRIDE, VAL_SIZE)\n",
    "test_out = np.array([test_data_raw.query(f\"TurbID == {tid}\")[cols_out].values for tid in test_data_raw['TurbID'].unique()], dtype='float32')\n",
    "\n",
    "train_in  = train_in[..., idx_cols_in]\n",
    "train_out = train_out[..., idx_cols_out]\n",
    "\n",
    "val_in    = val_in[..., idx_cols_in]\n",
    "val_out   = val_out[..., idx_cols_out]\n",
    "\n",
    "test_in   = test_in[..., idx_cols_in]\n",
    "\n",
    "print_shape(train_in, train_out, val_in, val_out, test_in, test_out)\n",
    "\n",
    "input_shape  = train_in.shape[1:]\n",
    "output_shape = train_out.shape[1:]\n",
    "print(\"input_shape:\", input_shape, \"output_shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf71cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Data split 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4d3ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from analysis_tools import preprocessing\n",
    "\n",
    "\n",
    "def drop_missing_data(X, y):\n",
    "    X_new, y_new = [], []\n",
    "    for X_s, y_s in zip(X, y):\n",
    "        if (np.isnan(X_s).sum() == 0) and (np.isnan(y_s).sum() == 0):\n",
    "            X_new.append(X_s)\n",
    "            y_new.append(y_s)\n",
    "    return np.array(X_new), np.array(y_new)\n",
    "\n",
    "def sliding_window(df, in_seq_len, out_seq_len, stride):\n",
    "    X_df = df.iloc[:-out_seq_len]\n",
    "    y_df = df.iloc[in_seq_len:]\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i in range(0, len(df) - in_seq_len - out_seq_len + 1, stride):\n",
    "        X = X_df.iloc[i:i + in_seq_len]\n",
    "        y = y_df.iloc[i:i + out_seq_len]\n",
    "        if (X.isnull().values.any()) or (y.isnull().values.any()):\n",
    "            continue\n",
    "        Xs.append(X.values)\n",
    "        ys.append(y.values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def Xy_from_dataframe(df, in_seq_len, out_seq_len, stride):\n",
    "    df = df.astype('float32')\n",
    "    if len(df) == in_seq_len:  # test dataframe\n",
    "        X = [df.values]  # expand_dim\n",
    "        y = None\n",
    "        X, _ = drop_missing_data(X, X)\n",
    "    else:\n",
    "        X, y = sliding_window(df, in_seq_len, out_seq_len, stride)\n",
    "        X, y = drop_missing_data(X, y)\n",
    "    return X, y\n",
    "\n",
    "cols   = data_sel.columns.drop(['Day', 'Time'])\n",
    "cols_X = data_sel.columns.drop(['Day', 'Time'])\n",
    "cols_y = ['Wspd', 'Patv']  # ['Etmp', 'Itmp', 'Prtv', 'Wspd_active', 'Wdir', 'Ndir', 'Pab', 'Patv']\n",
    "idx_cols_y = [i for i, col in enumerate(cols_X) if col in cols_y]\n",
    "\n",
    "IN_SEQ_LEN  = 2*144\n",
    "OUT_SEQ_LEN = 2*144\n",
    "STRIDE      = 2*144\n",
    "\n",
    "n_tids = data_sel['TurbID'].nunique()\n",
    "train_full_X, train_full_y = n_tids*[None], n_tids*[None]\n",
    "test_X,       test_y       = n_tids*[None], n_tids*[None]\n",
    "\n",
    "train_full_data = data_sel[data_sel['Day'] < 242]\n",
    "test_data       = data_sel[242 <= data_sel['Day']]\n",
    "\n",
    "for i, tid in tqdm(list(enumerate(data_sel['TurbID'].unique()))):\n",
    "    d_train_full = train_full_data.loc[train_full_data['TurbID'] == tid, cols]\n",
    "    d_test       = test_data.loc[test_data['TurbID'] == tid, cols]\n",
    "    \n",
    "    train_full_X[i], train_full_y[i] = Xy_from_dataframe(d_train_full, IN_SEQ_LEN, OUT_SEQ_LEN, STRIDE)\n",
    "    test_X[i],       _               = Xy_from_dataframe(d_test, IN_SEQ_LEN, OUT_SEQ_LEN, STRIDE)\n",
    "    test_y[i] = [test_data_raw.query(f\"TurbID == {tid}\")[['Wspd', 'Patv']].values]\n",
    "    \n",
    "train_full_X = np.concatenate(train_full_X)\n",
    "train_full_y = np.concatenate(train_full_y)[..., idx_cols_y]\n",
    "\n",
    "test_X = np.concatenate(test_X)\n",
    "test_y = np.concatenate(test_y)\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_full_X, train_full_y, test_size=0.1)\n",
    "\n",
    "print_shape(train_X, train_y, val_X, val_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33faa0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0c032",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_in = MinMaxScaler()\n",
    "scaler_in.fit(train_in.reshape(-1, train_in.shape[-1]))\n",
    "train_in_scale = scale(train_in, scaler_in)\n",
    "val_in_scale   = scale(val_in, scaler_in)\n",
    "test_in_scale  = scale(test_in, scaler_in)\n",
    "\n",
    "scaler_out = MinMaxScaler()\n",
    "scaler_out.fit(train_out.reshape(-1, train_out.shape[-1]))\n",
    "train_out_scale = scale(train_out, scaler_out)\n",
    "val_out_scale   = scale(val_out, scaler_out)\n",
    "test_out_scale  = scale(test_out, scaler_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f81a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from analysis_tools import modeling\n",
    "\n",
    "BATCH_SIZE  = 32\n",
    "SHUFFLE     = True\n",
    "\n",
    "train_ds = modeling.generate_dataset_tf(train_in_scale, train_out_scale, BATCH_SIZE, SHUFFLE)\n",
    "val_ds   = modeling.generate_dataset_tf(val_in_scale, val_out_scale, BATCH_SIZE, shuffle=False)\n",
    "test_ds  = modeling.generate_dataset_tf(test_in_scale, test_out_scale, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bde55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Transformer\n",
    "https://keras.io/examples/timeseries/timeseries_transformer_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7c3dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model_transformer(\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "#     x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(output_shape[-1])(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72866b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd6bea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_shape, units, n_blocks, dropout=None):\n",
    "    S,  D  = input_shape\n",
    "    S_, D_ = output_shape\n",
    "    \n",
    "    model = keras.Sequential(name=\"GRU-Model\") # Model\n",
    "    model.add(keras.Input(shape=(S, D), name='Input-Layer'))\n",
    "#     model.add(layers.Bidirectional(layers.GRU(units, return_sequences=False, dropout=dropout)))\n",
    "#     model.add(layers.RepeatVector(S_, name='Repeat-Vector-Layer'))\n",
    "    for _ in range(n_blocks):\n",
    "        model.add(layers.Bidirectional(layers.GRU(units, return_sequences=True, dropout=dropout)))\n",
    "    model.add(layers.Dense(units=D_))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def build_model_GRU(input_shape, output_shape, units=256, dropout=None):\n",
    "#     S,  D  = input_shape\n",
    "#     S_, D_ = output_shape\n",
    "#     model = keras.Sequential(name=\"GRU-Model\") # Model\n",
    "#     model.add(keras.Input(shape=(S, D), name='Input-Layer'))\n",
    "#     model.add(layers.Bidirectional(layers.GRU(units=units, activation='tanh', recurrent_activation='sigmoid', stateful=False, dropout=dropout), name='Hidden-GRU-Encoder-Layer'))\n",
    "#     model.add(layers.RepeatVector(S_, name='Repeat-Vector-Layer'))\n",
    "#     model.add(layers.Bidirectional(layers.GRU(units=units, activation='tanh', recurrent_activation='sigmoid', stateful=False, return_sequences=True, dropout=dropout), name='Hidden-GRU-Decoder-Layer'))\n",
    "#     model.add(layers.TimeDistributed(layers.Dense(units=D_), name='Output-Layer'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348858a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from livelossplot import PlotLossesKeras\n",
    "    \n",
    "class Loss(losses.Loss):\n",
    "    def __init__(self, loss_fn, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.loss_fn = loss_fn\n",
    "    def call(self, y_true, y_pred):\n",
    "        _, S, D = y_true.shape\n",
    "        res = y_true - y_pred\n",
    "        if self.loss_fn == 'rmse':\n",
    "            return tf.sqrt(tf.reduce_mean(tf.square(res)))\n",
    "        elif self.loss_fn == 'mse':\n",
    "            return tf.reduce_mean(tf.square(res))\n",
    "        elif self.loss_fn == 'mae':\n",
    "            return tf.reduce_mean(tf.abs(res))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            \n",
    "def compile_and_fit(model, train_ds, val_ds, epochs, patience_es=10, patience_lr=3):\n",
    "    model.compile('nadam', loss=Loss('rmse'), metrics=['mae'])\n",
    "    os.makedirs(PATH.ckpt, exist_ok=True)\n",
    "    return model.fit(train_ds, validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[\n",
    "                        PlotLossesKeras(),\n",
    "                        EarlyStopping(patience=patience_es, restore_best_weights=True),\n",
    "                        ReduceLROnPlateau(patience=patience_lr),\n",
    "                        ModelCheckpoint(join(PATH.ckpt, '[{epoch:03d} epoch].h5'), save_best_only=False, save_weights_only=True),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdacff2",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = build_model_transformer(input_shape, output_shape, head_size=8, num_heads=32, ff_dim=16, num_transformer_blocks=4, mlp_units=[64], dropout=0.3, mlp_dropout=0.3)\n",
    "model = build_model(input_shape, output_shape, units=128, n_blocks=2, dropout=0.1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2839238",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compile_and_fit(model, train_ds, val_ds, epochs=1000, patience_es=15, patience_lr=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d9e0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.load_weights(join(PATH.ckpt, \"[030 epoch].h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa919aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aece0b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Training, validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a913bba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633851d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(model, scaler_in, scaler_out, cols_in, cols_out, datasets, n_cols=5, figsize=(40, 20)):\n",
    "    def metric(y, p):\n",
    "        res = y-p\n",
    "        res = res[~np.isnan(res)]\n",
    "        return (np.abs(res).mean() + np.sqrt(((res)**2).mean()))/2\n",
    "    \n",
    "    for name, ds in datasets.items():\n",
    "        outs_scale = []\n",
    "        p_outs_scale = []\n",
    "        for in_scale, out_scale in ds.as_numpy_iterator():\n",
    "            p_out_scale = model.predict(in_scale, verbose=0)\n",
    "            outs_scale.append(out_scale);  p_outs_scale.append(p_out_scale)\n",
    "        \n",
    "        outs_scale = np.concatenate(outs_scale)\n",
    "        p_outs_scale = np.concatenate(p_outs_scale)\n",
    "\n",
    "        outs = inverse_scale(outs_scale, scaler_out)[..., -1].reshape(-1)\n",
    "        p_outs = inverse_scale(p_outs_scale, scaler_out)[..., -1].reshape(-1)\n",
    "        print(f\"[{name}] mean(MAE, RMSE): {metric(outs, p_outs):.2f}\")\n",
    "        \n",
    "        for inp, out in ds.take(1):\n",
    "            inp   = inp[:n_cols]\n",
    "            out   = out[:n_cols]\n",
    "            p_out = model.predict(inp, verbose=0)\n",
    "            \n",
    "        inps   = [pd.DataFrame(inverse_scale(d, scaler_in), columns=cols_in, dtype='float32') for d in inp]\n",
    "        outs   = [pd.DataFrame(inverse_scale(d, scaler_out), columns=cols_out) for d in out]\n",
    "        p_outs = [pd.DataFrame(inverse_scale(d, scaler_out), columns=cols_out) for d in p_out]\n",
    "        \n",
    "        n_rows = out.shape[-1]\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        for ax_col, col in zip(axes, cols_out):\n",
    "            for idx_row, (ax, out, p_out) in enumerate(zip(ax_col, outs, p_outs)):\n",
    "                ax.plot(out[col], label='true')\n",
    "                ax.plot(p_out[col], label=f'pred ({np.mean(abs(p_out[col]-out[col])):.3f})')\n",
    "                ax.fill_between(range(len(out[col])), out[col], p_out[col], alpha=0.1, color='b')\n",
    "                ax.set_xticklabels([])\n",
    "                ax.legend()\n",
    "                if idx_row == 0:\n",
    "                    ax.set_ylabel(col)\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62622bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_result(model, scaler_in, scaler_out, cols_in, cols_out, {'train': train_ds, 'val': val_ds}, figsize=(40, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd57cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## - Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f0cb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_result(model, scaler_in, scaler_out, cols_in, cols_out, {'test': test_ds}, figsize=(40, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_torch",
   "language": "python",
   "name": "tf_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}